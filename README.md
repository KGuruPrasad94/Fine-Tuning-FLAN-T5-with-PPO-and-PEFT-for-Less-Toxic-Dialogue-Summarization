Project Overview

This project explores Generative AI techniques for dialogue summarization using the FLAN-T5 model. Key focus areas include prompt engineering, model fine-tuning (including Parameter Efficient Fine-Tuning or PEFT), and reducing toxicity in generated content through Reinforcement Learning (PPO).

Learning Journey

1. Generative AI Use Case: Summarize Dialogue

	•	Objective: Understand the impact of prompt engineering on model outputs.
	•	Key Activities:
	•	Experimented with zero-shot, one-shot, and few-shot inferences.
	•	Adjusted generative configuration parameters.

2. Fine-Tuning FLAN-T5 for Enhanced Summarization

	•	Objective: Improve dialogue summarization through fine-tuning.
	•	Key Activities:
	•	Performed full fine-tuning of the FLAN-T5 model.
	•	Implemented Parameter Efficient Fine-Tuning (PEFT) using LoRA.
	•	Evaluated performance using ROUGE metrics and human feedback.

3. Reducing Toxicity with PPO

	•	Objective: Minimize toxicity in model-generated summaries.
	•	Key Activities:
	•	Fine-tuned the model using Proximal Policy Optimization (PPO) and Meta AI’s hate speech reward model.
	•	Evaluated and optimized for ethical AI behavior.

Key Concepts and Techniques

	•	Prompt Engineering: Crafting prompts to influence model outputs.
	•	Parameter Efficient Fine-Tuning (PEFT): Efficiently fine-tuning models with minimal resources.
	•	Proximal Policy Optimization (PPO): Applying reinforcement learning to optimize model behavior.
	•	ROUGE Metrics: Quantitative evaluation of summary quality.
